{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1677993664039
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pmaw in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.0.0)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pmaw) (2.28.2)\n",
            "Requirement already satisfied: praw in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pmaw) (7.7.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from praw->pmaw) (2.3.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from praw->pmaw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from praw->pmaw) (1.4.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (3.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pmaw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1678012159325
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pmaw import PushshiftAPI    #library Pushshift\n",
        "import datetime as dt            #library for date management\n",
        "import pandas as pd                         #library for data manipulation\n",
        "import matplotlib.pyplot as plt  #library for plotting\n",
        "import praw\n",
        "from IPython.display import display, clear_output\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1678012160530
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "api = PushshiftAPI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1678012162129
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def give_me_intervals(start_at, end_at, number_of_days_per_interval = 3):\n",
        "    \n",
        "    # end_at = math.ceil(datetime.utcnow().timestamp())\n",
        "        \n",
        "    ## 1 day = 86400,\n",
        "    period = (86400 * number_of_days_per_interval)\n",
        "    end = start_at + period\n",
        "    yield (int(start_at), int(end))\n",
        "    padding = 1\n",
        "    while end <= end_at:\n",
        "        start_at = end + padding\n",
        "        end = (start_at - padding) + period\n",
        "        yield int(start_at), int(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1678012164383
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"FOR COMMENTS\"\"\"\n",
        "def data_prep_comments(subreddit, start_time, end_time, filters, limit):\n",
        "    if(len(filters) == 0):\n",
        "        filters = ['id','title','author','created_utc',\n",
        "                   'body', 'url','score','upvote_ratio','ups','downs','permalink'\n",
        "                   ,'num_comments', 'link_id', 'comment_type', 'name', 'parent_id']                 \n",
        "                   #We set by default some useful columns\n",
        "    comments = []\n",
        "    comments_global = pd.DataFrame()\n",
        "    for interval in give_me_intervals(start_time,end_time,1):\n",
        "        clear_output(wait=True)\n",
        "        display('Retrieving Posts for ' + dt.datetime.utcfromtimestamp(interval[0]).strftime('%Y-%m-%d : %h : %M : %S') + \" to \" + dt.datetime.utcfromtimestamp(interval[1]).strftime('%Y-%m-%d : %h : %M : %S'))\n",
        "        comments = list(api.search_comments(\n",
        "            subreddit=subreddit,   #Subreddit we want to audit\n",
        "            after=interval[0],      #Start date\n",
        "            before=interval[1],       #End date\n",
        "            filter=filters,        #Column names we want to retrieve\n",
        "            limit=1000,\n",
        "            q = \"-body:[removed]\"))          ##Max number of posts\n",
        "        comments_local = pd.DataFrame(comments)\n",
        "        comments_global = pd.concat([comments_global, comments_local],ignore_index=True)\n",
        "\n",
        "\n",
        "    return comments_global #Return dataframe for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1678013230133
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Retrieving Posts for 2022-04-01 : Apr : 00 : 01 to 2022-04-02 : Apr : 00 : 00'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient\n",
        "import json\n",
        "\n",
        "subreddit = \"wallstreetbets\"     #Subreddit we are auditing\n",
        "conn_str = '...'\n",
        "container_name = '...'\n",
        "\n",
        "filters = []                     \n",
        "limit = 1000            \n",
        "for i in range(1, 13):\n",
        "    if i != 4:\n",
        "        start_time = int(dt.datetime(2022, i, 1).timestamp())\n",
        "        end_time = int(dt.datetime(2022, i+1, 1).timestamp())\n",
        "        df_p = data_prep_comments(subreddit,start_time,end_time,filters,limit) \n",
        "\n",
        "        # Define the connection string and blob information\n",
        "        blob_name = 'wallstreetbets_comments_' + str(i) +'.csv'\n",
        "\n",
        "        # Convert dataframe to CSV string\n",
        "        csv_string = df_p.to_csv(index=False, escapechar='\\\\')\n",
        "\n",
        "        # Create a BlobServiceClient object using the connection string\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
        "\n",
        "        # Get a BlobClient object for the blob\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "\n",
        "        # Upload the JSON string to the blob\n",
        "        blob_client.upload_blob(csv_string, overwrite=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1678014018804
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Retrieving Posts for 2023-04-01 : Apr : 00 : 01 to 2023-04-02 : Apr : 00 : 00'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient\n",
        "import json\n",
        "\n",
        "subreddit = \"wallstreetbets\"     #Subreddit we are auditing\n",
        "conn_str = '...'\n",
        "container_name = '...'\n",
        "\n",
        "filters = []                     \n",
        "limit = 1000            \n",
        "for i in range(1, 4):\n",
        "    if i != 4:\n",
        "        start_time = int(dt.datetime(2023, i, 1).timestamp())\n",
        "        end_time = int(dt.datetime(2023, i+1, 1).timestamp())\n",
        "        df_p = data_prep_comments(subreddit,start_time,end_time,filters,limit) \n",
        "\n",
        "        # Define the connection string and blob information\n",
        "        blob_name = 'wallstreetbets_comments_2023_' + str(i) +'.csv'\n",
        "\n",
        "        # Convert dataframe to CSV string\n",
        "        csv_string = df_p.to_csv(index=False, escapechar='\\\\')\n",
        "\n",
        "        # Create a BlobServiceClient object using the connection string\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
        "\n",
        "        # Get a BlobClient object for the blob\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "\n",
        "        # Upload the JSON string to the blob\n",
        "        blob_client.upload_blob(csv_string, overwrite=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "gather": {
          "logged": 1678015353065
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'etag': '\"0x8DB1D6BEA1EF880\"',\n",
              " 'last_modified': datetime.datetime(2023, 3, 5, 11, 22, 32, tzinfo=datetime.timezone.utc),\n",
              " 'content_md5': None,\n",
              " 'content_crc64': bytearray(b'+7\\xa8\\xa7\\x9fH\\x1f<'),\n",
              " 'client_request_id': '05ea3452-bb48-11ed-af25-6045bdbfeb1d',\n",
              " 'request_id': '8cd2e7c7-601e-005c-6954-4fa713000000',\n",
              " 'version': '2021-08-06',\n",
              " 'version_id': None,\n",
              " 'date': datetime.datetime(2023, 3, 5, 11, 22, 32, tzinfo=datetime.timezone.utc),\n",
              " 'request_server_encrypted': True,\n",
              " 'encryption_key_sha256': None,\n",
              " 'encryption_scope': None}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.storage.blob import BlobServiceClient\n",
        "import io\n",
        "\n",
        "# Connection string for your storage account\n",
        "conn_str = '...'\n",
        "\n",
        "# Blob service client\n",
        "blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
        "\n",
        "# List all blobs in the container/directory with .csv extension\n",
        "blob_list = blob_service_client.get_container_client(container_name).list_blob_names()\n",
        "\n",
        "csv_blobs = []\n",
        "for i in range(1, 10):\n",
        "    x = 'wallstreetbets_comments_' + str(i) + '.csv'\n",
        "    if x in blob_list:\n",
        "        csv_blobs.append(x)\n",
        "\n",
        "blob_list = blob_service_client.get_container_client(container_name).list_blob_names()\n",
        "for i in range(10, 14):\n",
        "    x = 'wallstreetbets_comments_' + str(i) + '.csv'\n",
        "    if x in blob_list:\n",
        "        csv_blobs.append(x)\n",
        "\n",
        "blob_list = blob_service_client.get_container_client(container_name).list_blob_names()\n",
        "for i in range(1, 4):\n",
        "    x = 'wallstreetbets_comments_2023_' + str(i) + '.csv'\n",
        "    if x in blob_list:\n",
        "        csv_blobs.append(x)\n",
        "\n",
        "# Read CSV files into a list of dataframes\n",
        "dfs = []\n",
        "for blob_name in csv_blobs:\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    stream_data = blob_client.download_blob().content_as_text()\n",
        "    df = pd.read_csv(io.StringIO(stream_data))\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "df_combined = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Write the combined dataframe to a CSV file in the blob storage\n",
        "output_blob = blob_service_client.get_blob_client(container=container_name, blob='comments_combined.csv')\n",
        "output_blob.upload_blob(df_combined.to_csv(index=False), overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1678017038434
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"FOR POSTS\"\"\"\n",
        "def data_prep_posts(subreddit, start_time, end_time, filters, limit):\n",
        "    if(len(filters) == 0):\n",
        "        filters = ['id','title','author','created_utc',\n",
        "                   'body', 'url','score','upvote_ratio','ups','downs','permalink'\n",
        "                   ,'num_comments', 'link_id', 'comment_type', 'name']                 \n",
        "                   #We set by default some useful columns\n",
        "    comments = []\n",
        "    comments_global = pd.DataFrame()\n",
        "    for interval in give_me_intervals(start_time,end_time,1):\n",
        "        clear_output(wait=True)\n",
        "        display('Retrieving Posts for ' + dt.datetime.utcfromtimestamp(interval[0]).strftime('%Y-%m-%d : %h : %M : %S') + \" to \" + dt.datetime.utcfromtimestamp(interval[1]).strftime('%Y-%m-%d : %h : %M : %S'))\n",
        "        comments = list(api.search_submissions(\n",
        "            subreddit=subreddit,   #Subreddit we want to audit\n",
        "            after=interval[0],      #Start date\n",
        "            before=interval[1],       #End date\n",
        "            filter=filters,        #Column names we want to retrieve\n",
        "            limit=None,\n",
        "            q = \"-body:[removed]\"))          ##Max number of posts\n",
        "        comments_local = pd.DataFrame(comments)\n",
        "        comments_global = pd.concat([comments_global, comments_local],ignore_index=True)\n",
        "\n",
        "\n",
        "    return comments_global #Return dataframe for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "gather": {
          "logged": 1678017792477
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Retrieving Posts for 2023-03-01 : Mar : 00 : 01 to 2023-03-02 : Mar : 00 : 00'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'etag': '\"0x8DB1D71981AA86E\"',\n",
              " 'last_modified': datetime.datetime(2023, 3, 5, 12, 3, 12, tzinfo=datetime.timezone.utc),\n",
              " 'content_md5': bytearray(b'\\xca2\\xd2<\\x87\\n\\xb5O\\xf2\\xe4>E)!\\xf7\\xcb'),\n",
              " 'client_request_id': 'b3c6f84e-bb4d-11ed-af25-6045bdbfeb1d',\n",
              " 'request_id': '8f968b0c-701e-0048-5c5a-4f6477000000',\n",
              " 'version': '2021-08-06',\n",
              " 'version_id': None,\n",
              " 'date': datetime.datetime(2023, 3, 5, 12, 3, 11, tzinfo=datetime.timezone.utc),\n",
              " 'request_server_encrypted': True,\n",
              " 'encryption_key_sha256': None,\n",
              " 'encryption_scope': None}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient\n",
        "import json\n",
        "\n",
        "subreddit = \"wallstreetbets\"     #Subreddit we are auditing\n",
        "conn_str = '...'\n",
        "container_name = '...'\n",
        "\n",
        "filters = []                     \n",
        "limit = 1000            \n",
        "start_time = int(dt.datetime(2023, 1, 1).timestamp())\n",
        "end_time = int(dt.datetime(2023, 3, 1).timestamp())\n",
        "df_p = data_prep_comments(subreddit,start_time,end_time,filters,limit) \n",
        "\n",
        "# Define the connection string and blob information\n",
        "blob_name = 'wallstreetbets_posts_2023.csv'\n",
        "\n",
        "# Convert dataframe to CSV string\n",
        "csv_string = df_p.to_csv(index=False, escapechar='\\\\')\n",
        "\n",
        "# Create a BlobServiceClient object using the connection string\n",
        "blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
        "\n",
        "# Get a BlobClient object for the blob\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "\n",
        "# Upload the JSON string to the blob\n",
        "blob_client.upload_blob(csv_string, overwrite=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
