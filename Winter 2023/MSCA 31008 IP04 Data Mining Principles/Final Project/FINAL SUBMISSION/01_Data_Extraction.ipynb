{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pmaw"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pmaw in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.0.0)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pmaw) (2.28.2)\nRequirement already satisfied: praw in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pmaw) (7.7.0)\nRequirement already satisfied: prawcore<3,>=2.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from praw->pmaw) (2.3.0)\nRequirement already satisfied: update-checker>=0.18 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from praw->pmaw) (0.18.0)\nRequirement already satisfied: websocket-client>=0.54.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from praw->pmaw) (1.4.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (2022.6.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->pmaw) (3.0.1)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1677993664039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pmaw import PushshiftAPI    #library Pushshift\r\n",
        "import datetime as dt            #library for date management\r\n",
        "import pandas as pd                         #library for data manipulation\r\n",
        "import matplotlib.pyplot as plt  #library for plotting\r\n",
        "import praw\r\n",
        "from IPython.display import display, clear_output\r\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678012159325
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = PushshiftAPI()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678012160530
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def give_me_intervals(start_at, end_at, number_of_days_per_interval = 3):\r\n",
        "    \r\n",
        "    # end_at = math.ceil(datetime.utcnow().timestamp())\r\n",
        "        \r\n",
        "    ## 1 day = 86400,\r\n",
        "    period = (86400 * number_of_days_per_interval)\r\n",
        "    end = start_at + period\r\n",
        "    yield (int(start_at), int(end))\r\n",
        "    padding = 1\r\n",
        "    while end <= end_at:\r\n",
        "        start_at = end + padding\r\n",
        "        end = (start_at - padding) + period\r\n",
        "        yield int(start_at), int(end)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678012162129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"FOR COMMENTS\"\"\"\r\n",
        "def data_prep_comments(subreddit, start_time, end_time, filters, limit):\r\n",
        "    if(len(filters) == 0):\r\n",
        "        filters = ['id','title','author','created_utc',\r\n",
        "                   'body', 'url','score','upvote_ratio','ups','downs','permalink'\r\n",
        "                   ,'num_comments', 'link_id', 'comment_type', 'name', 'parent_id']                 \r\n",
        "                   #We set by default some useful columns\r\n",
        "    comments = []\r\n",
        "    comments_global = pd.DataFrame()\r\n",
        "    for interval in give_me_intervals(start_time,end_time,1):\r\n",
        "        clear_output(wait=True)\r\n",
        "        display('Retrieving Posts for ' + dt.datetime.utcfromtimestamp(interval[0]).strftime('%Y-%m-%d : %h : %M : %S') + \" to \" + dt.datetime.utcfromtimestamp(interval[1]).strftime('%Y-%m-%d : %h : %M : %S'))\r\n",
        "        comments = list(api.search_comments(\r\n",
        "            subreddit=subreddit,   #Subreddit we want to audit\r\n",
        "            after=interval[0],      #Start date\r\n",
        "            before=interval[1],       #End date\r\n",
        "            filter=filters,        #Column names we want to retrieve\r\n",
        "            limit=1000,\r\n",
        "            q = \"-body:[removed]\"))          ##Max number of posts\r\n",
        "        comments_local = pd.DataFrame(comments)\r\n",
        "        comments_global = pd.concat([comments_global, comments_local],ignore_index=True)\r\n",
        "\r\n",
        "\r\n",
        "    return comments_global #Return dataframe for analysis"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678012164383
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient\r\n",
        "import json\r\n",
        "\r\n",
        "subreddit = \"wallstreetbets\"     #Subreddit we are auditing\r\n",
        "conn_str = 'DefaultEndpointsProtocol=https;AccountName=stocksentiment5092755591;AccountKey=JZxgix4PLYeU+KPo2i6Xuy8Sw/zulLC3riFSenydgAQpaaj2fajn+EIhb10cfc3kyi6g01Mamy3F+AStERwfMw==;EndpointSuffix=core.windows.net'\r\n",
        "container_name = 'azureml-blobstore-0d086a98-609c-4bba-93e6-d824f7a36850'\r\n",
        "\r\n",
        "filters = []                     \r\n",
        "limit = 1000            \r\n",
        "for i in range(1, 13):\r\n",
        "    if i != 4:\r\n",
        "        start_time = int(dt.datetime(2022, i, 1).timestamp())\r\n",
        "        end_time = int(dt.datetime(2022, i+1, 1).timestamp())\r\n",
        "        df_p = data_prep_comments(subreddit,start_time,end_time,filters,limit) \r\n",
        "\r\n",
        "        # Define the connection string and blob information\r\n",
        "        blob_name = 'wallstreetbets_comments_' + str(i) +'.csv'\r\n",
        "\r\n",
        "        # Convert dataframe to CSV string\r\n",
        "        csv_string = df_p.to_csv(index=False, escapechar='\\\\')\r\n",
        "\r\n",
        "        # Create a BlobServiceClient object using the connection string\r\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(conn_str)\r\n",
        "\r\n",
        "        # Get a BlobClient object for the blob\r\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\r\n",
        "\r\n",
        "        # Upload the JSON string to the blob\r\n",
        "        blob_client.upload_blob(csv_string, overwrite=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "'Retrieving Posts for 2022-04-01 : Apr : 00 : 01 to 2022-04-02 : Apr : 00 : 00'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678013230133
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient\r\n",
        "import json\r\n",
        "\r\n",
        "subreddit = \"wallstreetbets\"     #Subreddit we are auditing\r\n",
        "conn_str = 'DefaultEndpointsProtocol=https;AccountName=stocksentiment5092755591;AccountKey=JZxgix4PLYeU+KPo2i6Xuy8Sw/zulLC3riFSenydgAQpaaj2fajn+EIhb10cfc3kyi6g01Mamy3F+AStERwfMw==;EndpointSuffix=core.windows.net'\r\n",
        "container_name = 'azureml-blobstore-0d086a98-609c-4bba-93e6-d824f7a36850'\r\n",
        "\r\n",
        "filters = []                     \r\n",
        "limit = 1000            \r\n",
        "for i in range(1, 4):\r\n",
        "    if i != 4:\r\n",
        "        start_time = int(dt.datetime(2023, i, 1).timestamp())\r\n",
        "        end_time = int(dt.datetime(2023, i+1, 1).timestamp())\r\n",
        "        df_p = data_prep_comments(subreddit,start_time,end_time,filters,limit) \r\n",
        "\r\n",
        "        # Define the connection string and blob information\r\n",
        "        blob_name = 'wallstreetbets_comments_2023_' + str(i) +'.csv'\r\n",
        "\r\n",
        "        # Convert dataframe to CSV string\r\n",
        "        csv_string = df_p.to_csv(index=False, escapechar='\\\\')\r\n",
        "\r\n",
        "        # Create a BlobServiceClient object using the connection string\r\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(conn_str)\r\n",
        "\r\n",
        "        # Get a BlobClient object for the blob\r\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\r\n",
        "\r\n",
        "        # Upload the JSON string to the blob\r\n",
        "        blob_client.upload_blob(csv_string, overwrite=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "'Retrieving Posts for 2023-04-01 : Apr : 00 : 01 to 2023-04-02 : Apr : 00 : 00'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678014018804
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient\r\n",
        "import io\r\n",
        "\r\n",
        "# Connection string for your storage account\r\n",
        "conn_str = 'DefaultEndpointsProtocol=https;AccountName=stocksentiment5092755591;AccountKey=JZxgix4PLYeU+KPo2i6Xuy8Sw/zulLC3riFSenydgAQpaaj2fajn+EIhb10cfc3kyi6g01Mamy3F+AStERwfMw==;EndpointSuffix=core.windows.net'\r\n",
        "\r\n",
        "# Blob service client\r\n",
        "blob_service_client = BlobServiceClient.from_connection_string(conn_str)\r\n",
        "\r\n",
        "# List all blobs in the container/directory with .csv extension\r\n",
        "blob_list = blob_service_client.get_container_client(container_name).list_blob_names()\r\n",
        "\r\n",
        "csv_blobs = []\r\n",
        "for i in range(1, 10):\r\n",
        "    x = 'wallstreetbets_comments_' + str(i) + '.csv'\r\n",
        "    if x in blob_list:\r\n",
        "        csv_blobs.append(x)\r\n",
        "\r\n",
        "blob_list = blob_service_client.get_container_client(container_name).list_blob_names()\r\n",
        "for i in range(10, 14):\r\n",
        "    x = 'wallstreetbets_comments_' + str(i) + '.csv'\r\n",
        "    if x in blob_list:\r\n",
        "        csv_blobs.append(x)\r\n",
        "\r\n",
        "blob_list = blob_service_client.get_container_client(container_name).list_blob_names()\r\n",
        "for i in range(1, 4):\r\n",
        "    x = 'wallstreetbets_comments_2023_' + str(i) + '.csv'\r\n",
        "    if x in blob_list:\r\n",
        "        csv_blobs.append(x)\r\n",
        "\r\n",
        "# Read CSV files into a list of dataframes\r\n",
        "dfs = []\r\n",
        "for blob_name in csv_blobs:\r\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\r\n",
        "    stream_data = blob_client.download_blob().content_as_text()\r\n",
        "    df = pd.read_csv(io.StringIO(stream_data))\r\n",
        "    dfs.append(df)\r\n",
        "\r\n",
        "# Concatenate all dataframes into one\r\n",
        "df_combined = pd.concat(dfs, ignore_index=True)\r\n",
        "\r\n",
        "# Write the combined dataframe to a CSV file in the blob storage\r\n",
        "output_blob = blob_service_client.get_blob_client(container=container_name, blob='comments_combined.csv')\r\n",
        "output_blob.upload_blob(df_combined.to_csv(index=False), overwrite=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 55,
          "data": {
            "text/plain": "{'etag': '\"0x8DB1D6BEA1EF880\"',\n 'last_modified': datetime.datetime(2023, 3, 5, 11, 22, 32, tzinfo=datetime.timezone.utc),\n 'content_md5': None,\n 'content_crc64': bytearray(b'+7\\xa8\\xa7\\x9fH\\x1f<'),\n 'client_request_id': '05ea3452-bb48-11ed-af25-6045bdbfeb1d',\n 'request_id': '8cd2e7c7-601e-005c-6954-4fa713000000',\n 'version': '2021-08-06',\n 'version_id': None,\n 'date': datetime.datetime(2023, 3, 5, 11, 22, 32, tzinfo=datetime.timezone.utc),\n 'request_server_encrypted': True,\n 'encryption_key_sha256': None,\n 'encryption_scope': None}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678015353065
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"FOR POSTS\"\"\"\r\n",
        "def data_prep_posts(subreddit, start_time, end_time, filters, limit):\r\n",
        "    if(len(filters) == 0):\r\n",
        "        filters = ['id','title','author','created_utc',\r\n",
        "                   'body', 'url','score','upvote_ratio','ups','downs','permalink'\r\n",
        "                   ,'num_comments', 'link_id', 'comment_type', 'name']                 \r\n",
        "                   #We set by default some useful columns\r\n",
        "    comments = []\r\n",
        "    comments_global = pd.DataFrame()\r\n",
        "    for interval in give_me_intervals(start_time,end_time,1):\r\n",
        "        clear_output(wait=True)\r\n",
        "        display('Retrieving Posts for ' + dt.datetime.utcfromtimestamp(interval[0]).strftime('%Y-%m-%d : %h : %M : %S') + \" to \" + dt.datetime.utcfromtimestamp(interval[1]).strftime('%Y-%m-%d : %h : %M : %S'))\r\n",
        "        comments = list(api.search_submissions(\r\n",
        "            subreddit=subreddit,   #Subreddit we want to audit\r\n",
        "            after=interval[0],      #Start date\r\n",
        "            before=interval[1],       #End date\r\n",
        "            filter=filters,        #Column names we want to retrieve\r\n",
        "            limit=None,\r\n",
        "            q = \"-body:[removed]\"))          ##Max number of posts\r\n",
        "        comments_local = pd.DataFrame(comments)\r\n",
        "        comments_global = pd.concat([comments_global, comments_local],ignore_index=True)\r\n",
        "\r\n",
        "\r\n",
        "    return comments_global #Return dataframe for analysis"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678017038434
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient\r\n",
        "import json\r\n",
        "\r\n",
        "subreddit = \"wallstreetbets\"     #Subreddit we are auditing\r\n",
        "conn_str = 'DefaultEndpointsProtocol=https;AccountName=stocksentiment5092755591;AccountKey=JZxgix4PLYeU+KPo2i6Xuy8Sw/zulLC3riFSenydgAQpaaj2fajn+EIhb10cfc3kyi6g01Mamy3F+AStERwfMw==;EndpointSuffix=core.windows.net'\r\n",
        "container_name = 'azureml-blobstore-0d086a98-609c-4bba-93e6-d824f7a36850'\r\n",
        "\r\n",
        "filters = []                     \r\n",
        "limit = 1000            \r\n",
        "start_time = int(dt.datetime(2023, 1, 1).timestamp())\r\n",
        "end_time = int(dt.datetime(2023, 3, 1).timestamp())\r\n",
        "df_p = data_prep_comments(subreddit,start_time,end_time,filters,limit) \r\n",
        "\r\n",
        "# Define the connection string and blob information\r\n",
        "blob_name = 'wallstreetbets_posts_2023.csv'\r\n",
        "\r\n",
        "# Convert dataframe to CSV string\r\n",
        "csv_string = df_p.to_csv(index=False, escapechar='\\\\')\r\n",
        "\r\n",
        "# Create a BlobServiceClient object using the connection string\r\n",
        "blob_service_client = BlobServiceClient.from_connection_string(conn_str)\r\n",
        "\r\n",
        "# Get a BlobClient object for the blob\r\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\r\n",
        "\r\n",
        "# Upload the JSON string to the blob\r\n",
        "blob_client.upload_blob(csv_string, overwrite=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "'Retrieving Posts for 2023-03-01 : Mar : 00 : 01 to 2023-03-02 : Mar : 00 : 00'"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 59,
          "data": {
            "text/plain": "{'etag': '\"0x8DB1D71981AA86E\"',\n 'last_modified': datetime.datetime(2023, 3, 5, 12, 3, 12, tzinfo=datetime.timezone.utc),\n 'content_md5': bytearray(b'\\xca2\\xd2<\\x87\\n\\xb5O\\xf2\\xe4>E)!\\xf7\\xcb'),\n 'client_request_id': 'b3c6f84e-bb4d-11ed-af25-6045bdbfeb1d',\n 'request_id': '8f968b0c-701e-0048-5c5a-4f6477000000',\n 'version': '2021-08-06',\n 'version_id': None,\n 'date': datetime.datetime(2023, 3, 5, 12, 3, 11, tzinfo=datetime.timezone.utc),\n 'request_server_encrypted': True,\n 'encryption_key_sha256': None,\n 'encryption_scope': None}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1678017792477
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}